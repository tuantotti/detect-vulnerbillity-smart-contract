{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6476634,"sourceType":"datasetVersion","datasetId":3740786}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import and connect to gg drive","metadata":{"id":"C2lOX1Sr0iSD"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount(\"/content/drive\")","metadata":{"id":"diMysT7u0l9M","outputId":"58566556-480a-4529-ddf9-19e8b1b2db12","execution":{"iopub.status.busy":"2023-11-28T03:18:10.708932Z","iopub.execute_input":"2023-11-28T03:18:10.709397Z","iopub.status.idle":"2023-11-28T03:18:10.714021Z","shell.execute_reply.started":"2023-11-28T03:18:10.709350Z","shell.execute_reply":"2023-11-28T03:18:10.712844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cd /content/drive/MyDrive/lab/","metadata":{"id":"q-E2Mi_30otZ","outputId":"ae9bc397-fff3-4883-8ea0-0cd1a7207007","execution":{"iopub.status.busy":"2023-11-28T03:18:10.716048Z","iopub.execute_input":"2023-11-28T03:18:10.716371Z","iopub.status.idle":"2023-11-28T03:18:10.727790Z","shell.execute_reply.started":"2023-11-28T03:18:10.716345Z","shell.execute_reply":"2023-11-28T03:18:10.726781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import package","metadata":{"id":"7ShsVEooMxRM"}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport collections\nimport numpy as np\nimport zipfile\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.utils.data import TensorDataset, DataLoader\nimport time","metadata":{"id":"_fJlPHWD0qcl","execution":{"iopub.status.busy":"2023-11-28T03:18:10.728990Z","iopub.execute_input":"2023-11-28T03:18:10.729348Z","iopub.status.idle":"2023-11-28T03:18:13.141125Z","shell.execute_reply.started":"2023-11-28T03:18:10.729321Z","shell.execute_reply":"2023-11-28T03:18:13.139984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-11-28T03:18:13.142626Z","iopub.execute_input":"2023-11-28T03:18:13.143058Z","iopub.status.idle":"2023-11-28T03:18:13.167251Z","shell.execute_reply.started":"2023-11-28T03:18:13.143032Z","shell.execute_reply":"2023-11-28T03:18:13.166388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n dev = \"cuda:0\"\nelse:\n dev = \"cpu\"\ndevice = torch.device(dev)\ndevice","metadata":{"id":"ZlwZK-zl-Lfr","outputId":"97917afe-b9f8-478d-f333-69e0600c3937","execution":{"iopub.status.busy":"2023-11-28T03:18:13.170549Z","iopub.execute_input":"2023-11-28T03:18:13.170930Z","iopub.status.idle":"2023-11-28T03:18:13.202606Z","shell.execute_reply.started":"2023-11-28T03:18:13.170895Z","shell.execute_reply":"2023-11-28T03:18:13.201617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save classification report","metadata":{"id":"x2mdCL9NM5S1"}},{"cell_type":"code","source":"def save_classification(y_test, y_pred, out_dir, labels):\n  if isinstance(y_pred, np.ndarray) == False:\n    y_pred = y_pred.toarray()\n\n  def accuracy(y_true, y_pred):\n    temp = 0\n    for i in range(y_true.shape[0]):\n        numerator = sum(np.logical_and(y_true[i], y_pred[i]))\n        denominator = sum(np.logical_or(y_true[i], y_pred[i]))\n        if denominator != 0:\n          temp += numerator / denominator\n    return temp / y_true.shape[0]\n\n  out = classification_report(y_test,y_pred, output_dict=True, target_names=labels)\n  total_support = out['samples avg']['support']\n\n  mr = accuracy_score(y_test, y_pred)\n  acc = accuracy(y_test,y_pred)\n  hm = hamming_loss(y_test, y_pred)\n\n  out['Exact Match Ratio'] = {'precision': mr, 'recall': mr, 'f1-score': mr, 'support': total_support}\n  out['Hamming Loss'] = {'precision': hm, 'recall': hm, 'f1-score': hm, 'support': total_support}\n  out['Accuracy'] = {'precision': acc, 'recall': acc, 'f1-score': acc, 'support': total_support}\n  out_df = pd.DataFrame(out).transpose()\n  print(out_df)\n\n  out_df.to_csv(out_dir)\n\n  return out_df","metadata":{"id":"cqJSks2dp6QU","execution":{"iopub.status.busy":"2023-11-28T03:18:13.203968Z","iopub.execute_input":"2023-11-28T03:18:13.204366Z","iopub.status.idle":"2023-11-28T03:18:13.215021Z","shell.execute_reply.started":"2023-11-28T03:18:13.204331Z","shell.execute_reply":"2023-11-28T03:18:13.214095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{"id":"FAQOzGAG1XRr"}},{"cell_type":"code","source":"class Tokenizer(object):\n    def __init__(self, num_words=None, lower=True) -> None:\n        self.word_index = {}\n        self.word_counts = {}\n        self.num_words = num_words\n        self.split = \" \"\n        self.lower = lower\n\n    def fit_on_texts(self, texts):\n        \"\"\"\n        create vocabulary\n\n        Args:\n            text: list of strings or list of list of strings\n        \"\"\"\n        for text in texts:\n            seq = self.text_to_word_sequence(text)\n            for w in seq:\n                if w in self.word_counts:\n                    self.word_counts[w] += 1\n\n                else:\n                    self.word_counts[w] = 1\n        vocab = self.word_counts.keys()\n        self.word_index = dict(zip(vocab, list(range(1, len(vocab) + 1))))\n\n    def text_to_word_sequence(self, input_text):\n        if self.lower == True:\n            input_text = input_text.lower()\n\n        seq = input_text.split(self.split)\n        return seq\n\n    def texts_to_sequences(self, texts):\n        return list(self.texts_to_sequences_generator(texts))\n\n    def texts_to_sequences_generator(self, texts):\n        for text in texts:\n            seq = self.text_to_word_sequence(text)\n            vect = []\n            for w in seq:\n                i = self.word_index.get(w)\n                vect.append(i)\n            yield vect\n\ndef pad_sequences(\n    sequences,\n    maxlen=None,\n    dtype=\"int32\",\n    padding=\"pre\",\n    truncating=\"pre\",\n    value=0.0\n):\n    \"\"\"\n    Args:\n        sequences: List of sequences (each sequence is a list of integers).\n        maxlen: Optional Int, maximum length of all sequences. If not provided,\n            sequences will be padded to the length of the longest individual\n            sequence.\n        dtype: (Optional, defaults to `\"int32\"`). Type of the output sequences.\n            To pad sequences with variable length strings, you can use `object`.\n        padding: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n            pad either before or after each sequence.\n        truncating: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n            remove values from sequences larger than\n            `maxlen`, either at the beginning or at the end of the sequences.\n        value: Float or String, padding value. (Optional, defaults to 0.)\n\n    Returns:\n        Numpy array with shape `(len(sequences), maxlen)`\n\n    Raises:\n        ValueError: In case of invalid values for `truncating` or `padding`,\n            or in case of invalid shape for a `sequences` entry.\n    \"\"\"\n\n    if not hasattr(sequences, \"__len__\"):\n        raise ValueError(\"`sequences` must be iterable.\")\n    num_samples = len(sequences)\n\n    lengths = []\n    sample_shape = ()\n    flag = True\n\n    # take the sample shape from the first non empty sequence\n    # checking for consistency in the main loop below.\n\n    for x in sequences:\n        try:\n            lengths.append(len(x))\n            if flag and len(x):\n                sample_shape = np.asarray(x).shape[1:]\n                flag = False\n        except TypeError as e:\n            raise ValueError(\n                \"`sequences` must be a list of iterables. \"\n                f\"Found non-iterable: {str(x)}\"\n            ) from e\n\n    if maxlen is None:\n        maxlen = np.max(lengths)\n\n    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(\n        dtype, np.unicode_\n    )\n    if isinstance(value, str) and dtype != object and not is_dtype_str:\n        raise ValueError(\n            f\"`dtype` {dtype} is not compatible with `value`'s type: \"\n            f\"{type(value)}\\nYou should set `dtype=object` for variable length \"\n            \"strings.\"\n        )\n\n    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n    for idx, s in enumerate(sequences):\n        if not len(s):\n            continue  # empty list/array was found\n        if truncating == \"pre\":\n            trunc = s[-maxlen:]\n        elif truncating == \"post\":\n            trunc = s[:maxlen]\n        else:\n            raise ValueError(f'Truncating type \"{truncating}\" not understood')\n\n        # check `trunc` has expected shape\n        trunc = np.asarray(trunc, dtype=dtype)\n        if trunc.shape[1:] != sample_shape:\n            raise ValueError(\n                f\"Shape of sample {trunc.shape[1:]} of sequence at \"\n                f\"position {idx} is different from expected shape \"\n                f\"{sample_shape}\"\n            )\n\n        if padding == \"post\":\n            x[idx, : len(trunc)] = trunc\n        elif padding == \"pre\":\n            x[idx, -len(trunc) :] = trunc\n        else:\n            raise ValueError(f'Padding type \"{padding}\" not understood')\n    return x","metadata":{"id":"8wki3TAqGFYM","execution":{"iopub.status.busy":"2023-11-28T03:18:13.216471Z","iopub.execute_input":"2023-11-28T03:18:13.216760Z","iopub.status.idle":"2023-11-28T03:18:13.239387Z","shell.execute_reply.started":"2023-11-28T03:18:13.216735Z","shell.execute_reply":"2023-11-28T03:18:13.238369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create DataLoader","metadata":{"id":"1q9IdP92n3xT"}},{"cell_type":"markdown","source":"# Define Model","metadata":{"id":"NvxJELKhn7ny"}},{"cell_type":"code","source":"class Branch(nn.Module):\n  def __init__(self, input_size, hidden_size, dropout, num_outputs):\n    super(Branch, self).__init__()\n\n    self.dense1 = nn.Linear(input_size, hidden_size)\n    self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n    self.dropout = nn.Dropout(p=dropout)\n    self.dense2 = nn.Linear(hidden_size, num_outputs)\n\n  def forward(self, x):\n    # print(\"Branch Input Shape:\", x.shape)\n    out_dense1 = self.dense1(x)\n    # print(\"After Dense1 Shape:\", out_dense1.shape)\n    out_batchnorm1 = self.batchnorm1(out_dense1)\n    out_dropout = self.dropout(out_batchnorm1)\n    out_dense2 = self.dense2(out_dropout)\n\n    return out_dense2","metadata":{"execution":{"iopub.status.busy":"2023-11-28T03:18:13.240816Z","iopub.execute_input":"2023-11-28T03:18:13.241225Z","iopub.status.idle":"2023-11-28T03:18:13.253624Z","shell.execute_reply.started":"2023-11-28T03:18:13.241190Z","shell.execute_reply":"2023-11-28T03:18:13.252795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Escort(nn.Module):\n  def __init__(self, vocab_size, embedd_size, rnn_hidden_size, n_layers, num_classes, method_type, bidirectional, is_multibranches):\n    super(Escort, self).__init__()\n    \n    self.word_embeddings = nn.Embedding(vocab_size, embedd_size, padding_idx=0)\n    self.bidirectional = bidirectional\n    self.is_multibranches = is_multibranches\n    if method_type=='GRU':\n        self.rnn = nn.GRU(embedd_size, rnn_hidden_size, num_layers=n_layers, batch_first=True, bidirectional=self.bidirectional)\n    else:\n        self.rnn = nn.LSTM(embedd_size, rnn_hidden_size, num_layers=n_layers, batch_first=True, bidirectional=self.bidirectional)\n    \n    if self.is_multibranches:\n        self.branches = nn.ModuleList([Branch(rnn_hidden_size, 128, 0.2, 1) for _ in range(num_classes)])\n    else:\n        self.branch = Branch(rnn_hidden_size, 128, 0.2, num_classes)\n        \n    self.sigmoid = nn.Sigmoid()\n\n  def forward(self, sequence):\n    # print(\"Input to Escort:\", sequence.shape)\n    embeds = self.word_embeddings(sequence)\n    rnn_out, hidden = self.rnn(embeds)\n    \n    if self.bidirectional:\n        rnn_out = (rnn_out[:, :, :self.rnn_hidden_size] + rnn_out[:, :, self.rnn_hidden_size:])\n    \n    if self.is_multibranches:\n        out_branch = [branch(rnn_out[:, -1, :]) for branch in self.branches]\n        out_branch = torch.cat(output_branches, dim=1)\n    else:\n        out_branch = self.branch(rnn_out[:, -1, :])\n    outputs = self.sigmoid(out_branch)\n    return outputs","metadata":{"id":"N95V8a_125ys","execution":{"iopub.status.busy":"2023-11-28T03:18:13.254686Z","iopub.execute_input":"2023-11-28T03:18:13.254941Z","iopub.status.idle":"2023-11-28T03:18:13.269074Z","shell.execute_reply.started":"2023-11-28T03:18:13.254920Z","shell.execute_reply":"2023-11-28T03:18:13.268102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainer","metadata":{"id":"3BAJUkZcoAyE"}},{"cell_type":"markdown","source":"## Train and Validation Steps","metadata":{"id":"7vxHiIbwrKRS"}},{"cell_type":"code","source":"def calculate_score(y_true, preds):\n    acc_score = accuracy_score(y_true, preds)\n\n    return acc_score\n\ndef train_steps(training_loader, model, loss_f, optimizer):\n    training_loss = 0\n    n_correct = 0\n    nb_tr_steps = 0\n    nb_tr_examples = 0\n    train_acc = 0.\n\n    model.train()\n    for step, batch in enumerate(training_loader):\n        # push the batch to gpu\n        inputs = batch[0].to(device)\n        labels = batch[1].to(device)\n\n        preds = model(inputs)\n\n        loss = loss_f(preds, labels)\n        training_loss += loss.item()\n\n        preds = preds.detach().cpu().numpy()\n        preds = np.where(preds>=0.5, 1, 0)\n        labels = labels.to('cpu').numpy()\n\n        acc_score = calculate_score(labels, preds)\n        train_acc += acc_score\n\n        nb_tr_steps += 1\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # When using GPU\n        optimizer.step()\n\n    epoch_loss = training_loss / nb_tr_steps\n    epoch_acc = train_acc / nb_tr_steps\n    return epoch_loss, epoch_acc\n\ndef evaluate_steps(validating_loader, model, loss_f):\n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_preds = []\n    total_labels = []\n    # iterate over batches\n    for step, batch in enumerate(validating_loader):\n        # push the batch to gpu\n        inputs = batch[0].to(device)\n        labels = batch[1].to(device)\n\n        # deactivate autograd\n        with torch.no_grad():\n            # model predictions\n            preds = model(inputs)\n\n            # compute the validation loss between actual and predicted values\n            loss = loss_f(preds, labels)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n            preds = np.where(preds>=0.5, 1, 0)\n            total_preds += list(preds)\n            total_labels += labels.tolist()\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(validating_loader)\n    acc_score = calculate_score(total_labels, total_preds)\n\n    return avg_loss, acc_score","metadata":{"id":"UF8lqT-M-BL6","execution":{"iopub.status.busy":"2023-11-28T03:18:13.270432Z","iopub.execute_input":"2023-11-28T03:18:13.271044Z","iopub.status.idle":"2023-11-28T03:18:13.284705Z","shell.execute_reply.started":"2023-11-28T03:18:13.271017Z","shell.execute_reply":"2023-11-28T03:18:13.283671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training loop","metadata":{"id":"sZ9uzmMGsOhR"}},{"cell_type":"code","source":"def train(epochs, model, optimizer, criterion, out_dir):\n  # empty lists to store training and validation loss of each epoch\n  # set initial loss to infinite\n  best_valid_loss = float('inf')\n  train_losses = []\n  valid_losses = []\n  train_accuracies = []\n  valid_accuracies = []\n\n  for epoch in range(epochs):\n    start_time = time.time()\n    train_loss, train_acc = train_steps(data_train_loader, model, criterion, optimizer)\n    valid_loss, valid_acc = evaluate_steps(data_val_loader, model, criterion)\n\n    # save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), out_dir)\n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    train_accuracies.append(train_acc)\n    valid_accuracies.append(valid_acc)\n\n    elapsed_time = time.time() - start_time\n\n    print('Epoch {}/{} \\t loss={:.4f} \\t accuracy={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(epoch + 1, epochs, train_loss, train_acc, valid_loss, valid_acc, elapsed_time))\n  return train_accuracies, valid_accuracies, train_losses, valid_losses","metadata":{"id":"rQ82Aqtbpifd","execution":{"iopub.status.busy":"2023-11-28T03:18:13.286114Z","iopub.execute_input":"2023-11-28T03:18:13.286437Z","iopub.status.idle":"2023-11-28T03:18:13.298967Z","shell.execute_reply.started":"2023-11-28T03:18:13.286411Z","shell.execute_reply":"2023-11-28T03:18:13.298035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_graph(epochs, train, valid, tittle):\n    fig = plt.figure(figsize=(12,12))\n    plt.title(tittle)\n    plt.plot(list(np.arange(epochs) + 1) , train, label='train')\n    plt.plot(list(np.arange(epochs) + 1), valid, label='validation')\n    plt.xlabel('num_epochs', fontsize=12)\n    plt.ylabel('loss', fontsize=12)\n    plt.legend(loc='best')\n","metadata":{"id":"yNXt80cVsWgx","execution":{"iopub.status.busy":"2023-11-28T03:18:13.300326Z","iopub.execute_input":"2023-11-28T03:18:13.300673Z","iopub.status.idle":"2023-11-28T03:18:13.312422Z","shell.execute_reply.started":"2023-11-28T03:18:13.300645Z","shell.execute_reply":"2023-11-28T03:18:13.311434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Model","metadata":{"id":"VXnRPRFDwxle"}},{"cell_type":"code","source":"def predict(testing_loader, model, loss_f):\n    # deactivate dropout layers\n    model.eval()\n\n    # empty list to save the model predictions\n    total_preds = []\n    total_labels = []\n    start_time = time.time()\n    # iterate over batches\n    for step, batch in enumerate(testing_loader):\n        # push the batch to gpu\n        inputs = batch[0].to(device)\n        labels = batch[1].to(device)\n\n        # deactivate autograd\n        with torch.no_grad():\n            # model predictions\n            preds = model(inputs)\n\n            preds = preds.detach().cpu().numpy()\n            preds = np.where(preds>=0.5, 1, 0)\n            total_preds += list(preds)\n            total_labels += labels.tolist()\n\n    execution_time = (time.time() - start_time) / len(total_labels)\n    return total_preds, total_labels, execution_time","metadata":{"id":"soXxXY8yw3Er","execution":{"iopub.status.busy":"2023-11-28T03:18:13.313887Z","iopub.execute_input":"2023-11-28T03:18:13.314858Z","iopub.status.idle":"2023-11-28T03:18:13.324142Z","shell.execute_reply.started":"2023-11-28T03:18:13.314822Z","shell.execute_reply":"2023-11-28T03:18:13.323195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run","metadata":{"id":"9H2Jpz4Qx0MC"}},{"cell_type":"code","source":"\n# data_folder = os.getcwd()+'/Wisdomnet/Untitled Folder/'\n# out_folder = os.getcwd()+'/trained/'\ndata_folder='/kaggle/input/opcode/'\nout_folder ='/kaggle/working/'\n# Read data\nX_train = pd.read_csv(data_folder+'X_train.csv')['BYTECODE'].to_numpy()\nX_test = pd.read_csv(data_folder+'X_test.csv')['BYTECODE'].to_numpy()\nX_val = pd.read_csv(data_folder+'X_val.csv')['BYTECODE'].to_numpy()\n\ny_train = pd.read_csv(data_folder+'y_train.csv').to_numpy()\ny_test = pd.read_csv(data_folder+'y_test.csv').to_numpy()\ny_val = pd.read_csv(data_folder+'y_val.csv').to_numpy()\n\n# Create tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(texts=X_train)\nsequences_train = tokenizer.texts_to_sequences(texts=X_train)\nsequences_test = tokenizer.texts_to_sequences(texts=X_test)\nsequences_val = tokenizer.texts_to_sequences(texts=X_val)\n\n# Define constant\ninput_size = 4100\nepochs = 20\nSIZE_OF_VOCAB = len(tokenizer.word_index.keys())\nEMBEDDED_SIZE = 5\nGRU_HIDDEN_SIZE = 64\nNUM_OUTPUT_NODES = 4\nNUM_LAYERS = 1\nDROPOUT = 0.2\nRNN_TYPE = 'GRU'\nBIDIRECTIONAL = True\nUSE_MULTIBRANCHES = True\nlabels = ['Timestamp dependence', 'Outdated Solidity version', 'Frozen Ether', 'Delegatecall Injection']\nbatch_size = 32\n\n# Prepare data for training, evaluating, testing\nX_train = pad_sequences(sequences_train, maxlen=input_size)\nX_val = pad_sequences(sequences_val, maxlen=input_size)\nX_test = pad_sequences(sequences_test, maxlen=input_size)","metadata":{"id":"CdrqOWprL6G1","execution":{"iopub.status.busy":"2023-11-28T03:18:13.327621Z","iopub.execute_input":"2023-11-28T03:18:13.327916Z","iopub.status.idle":"2023-11-28T03:20:47.592085Z","shell.execute_reply.started":"2023-11-28T03:18:13.327892Z","shell.execute_reply":"2023-11-28T03:20:47.590702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensor_X_train = torch.tensor(X_train)\ntensor_X_val = torch.tensor(X_val)\ntensor_X_test = torch.tensor(X_test)\ntensor_Y_train = torch.FloatTensor(y_train)\ntensor_Y_val = torch.FloatTensor(y_val)\ntensor_Y_test = torch.FloatTensor(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T03:20:47.593015Z","iopub.status.idle":"2023-11-28T03:20:47.593565Z","shell.execute_reply.started":"2023-11-28T03:20:47.593298Z","shell.execute_reply":"2023-11-28T03:20:47.593325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = TensorDataset(tensor_X_train, tensor_Y_train)\nval_dataset = TensorDataset(tensor_X_val, tensor_Y_val)\ntest_dataset = TensorDataset(tensor_X_test, tensor_Y_test)\n\ndata_train_loader = DataLoader(train_dataset, batch_size=batch_size)\ndata_val_loader = DataLoader(val_dataset, batch_size=batch_size)\ndata_test_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T03:20:47.594832Z","iopub.status.idle":"2023-11-28T03:20:47.595332Z","shell.execute_reply.started":"2023-11-28T03:20:47.595058Z","shell.execute_reply":"2023-11-28T03:20:47.595081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define model\nmodel = Escort(SIZE_OF_VOCAB, EMBEDDED_SIZE, GRU_HIDDEN_SIZE, NUM_LAYERS, NUM_OUTPUT_NODES, RNN_TYPE, BIDIRECTIONAL, USE_MULTIBRANCHES)\nmodel.to(device)\nprint(model)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.BCELoss()","metadata":{"id":"vTM0v9sK3nCZ","execution":{"iopub.status.busy":"2023-11-28T03:20:47.596942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\ntrain_accuracies, valid_accuracies, train_losses, valid_losses = train(epochs, model, optimizer, criterion, out_folder+'/gru-escort.pt')","metadata":{"id":"HE-t7X_TMf6m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot graph\nplot_graph(epochs, train_losses, valid_losses, \"Train/Validation Loss\")\nplot_graph(epochs, train_accuracies, valid_accuracies, \"Train/Validation Accuracy\")","metadata":{"id":"n0ftnFQa3k34","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test model\ntotal_preds, total_labels, execution_time = predict(data_test_loader, model, criterion)","metadata":{"id":"pELUFirP3Kcx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(execution_time)","metadata":{"id":"SkKf6FVD5NVW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_classification(y_pred=np.array(total_preds), y_test=np.array(total_labels), labels=labels, out_dir='escort.csv')","metadata":{"id":"KoxRi-3x4JGY","trusted":true},"execution_count":null,"outputs":[]}]}